update_counts <- update_counts %>% select(-abund) %>% mutate(counts = counts - num_cells_lost) %>% mutate(abund = counts/sum(counts) ) %>% select(-num_cells_lost)
print(paste("total number of cells after replating = ",sum(SortOut$counts)))
### SORT CELLS
# (1) Setup initial cell count matrix with number of cells equal to the number of cells sorted
OGcounts <- data.frame("Cell_ID"= c(seq(1,library_size))) %>%                             # create matrix with cells equal to sorted library size
add_column(    gr = rnorm(nrow(.), mean=mean_growth_rate, sd=sd_growth_rate), # sample growth rates from normal distribution
counts = c(rep(1,library_size)) )                                  # set initial count of each subclone to 1
OGcounts <- OGcounts %>% mutate(abund= counts/sum(OGcounts$counts))                       # add new column for subclonal abundance
# (2) Stochastic selection of cells that die post-cell sort
cells_lost_sort = sample(    x = OGcounts$Cell_ID,                                        # choose from set of Cell_IDs in data frame
size = round(prob_cellsort_death*sum(OGcounts$counts)),         # select number of cells equal to current number of cells multiplied by probability of death post cell sort (prob_cellsort_death)
prob = OGcounts$abund )                                         # weight probability of Cell_ID selection by abundance
SortMatrix <- OGcounts[OGcounts$Cell_ID %not in% cells_lost_sort, ]                       # store selected cells in new data frame
# (3) Determine time to research ~50k cells from starting number of cells post-sort
SortMax_gr = max(SortMatrix$gr)                                                           # calculate max growth rate of selected cells
num_uniq = length(unique(SortMatrix$Cell_ID))
t_expand = solvet(g = SortMax_gr, N0 = sum(SortMatrix$counts), Nfinal = 0.85*k_96well, k = k_96well)   # solve logistic growth equation for expansion time
### EXPAND CELLS
# (4) Simulate growth of each subclone to t_expand and store new cell counts and abundance in data frame
SortGrow <- SortMatrix %>% select(-abund) %>%
mutate(counts = floor(solveN(g = gr, N0 = counts, tf = t_expand, k = k_96well)), # solve logistic growth for N at expansion time for each Cell_ID
abund = counts/sum(counts))                                              # determine abundance of each Cell_ID in population
head(SortGrow)
print(paste("total number of cells after expansion = ",sum(SortGrow$counts)))
# (5) Stochastic selection of cells that die during expansion to new vessel
cells_lost_expand = sample(    x = SortGrow$Cell_ID,                                     # choose from set of Cell_IDs in data frame
size = round(prob_passage_death*sum(SortGrow$counts)),         # select number of cells equal to current number of cells multiplied by probability of death post cell sort (prob_cellsort_death)
prob = SortGrow$abund,                                         # weight probability of Cell_ID selection by abundance
replace = TRUE)
#SortOut <- SortGrow[SortGrow$Cell_ID %not in% cells_lost_expand, ]                       # store selected cells in new data frame
# count occurences of each cell_ID in cells_lost_expand
num_lost <- data.frame(table(cells_lost_expand))                 # count occurrences of each cell id
colnames(num_lost) <- c('Cell_ID', 'num_cells_lost')             # rename columns
num_lost$Cell_ID <- as.integer(as.character(num_lost$Cell_ID)) # change data type of cell_id column to intege
update_counts <- left_join(SortGrow, num_lost)
update_counts$num_cells_lost[is.na(update_counts$num_cells_lost)] <- 0
update_counts <- update_counts %>% select(-abund) %>% mutate(counts = counts - num_cells_lost) %>% mutate(abund = counts/sum(counts) ) %>% select(-num_cells_lost)
print(paste("total number of cells after replating = ",sum(SortOut$counts)))
SortOut <- update_counts %>% select(-abund) %>% mutate(counts = counts - num_cells_lost) %>% mutate(abund = counts/sum(counts) ) %>% select(-num_cells_lost)
### SORT CELLS
# (1) Setup initial cell count matrix with number of cells equal to the number of cells sorted
OGcounts <- data.frame("Cell_ID"= c(seq(1,library_size))) %>%                             # create matrix with cells equal to sorted library size
add_column(    gr = rnorm(nrow(.), mean=mean_growth_rate, sd=sd_growth_rate), # sample growth rates from normal distribution
counts = c(rep(1,library_size)) )                                  # set initial count of each subclone to 1
OGcounts <- OGcounts %>% mutate(abund= counts/sum(OGcounts$counts))                       # add new column for subclonal abundance
# (2) Stochastic selection of cells that die post-cell sort
cells_lost_sort = sample(    x = OGcounts$Cell_ID,                                        # choose from set of Cell_IDs in data frame
size = round(prob_cellsort_death*sum(OGcounts$counts)),         # select number of cells equal to current number of cells multiplied by probability of death post cell sort (prob_cellsort_death)
prob = OGcounts$abund )                                         # weight probability of Cell_ID selection by abundance
SortMatrix <- OGcounts[OGcounts$Cell_ID %not in% cells_lost_sort, ]                       # store selected cells in new data frame
# (3) Determine time to research ~50k cells from starting number of cells post-sort
SortMax_gr = max(SortMatrix$gr)                                                           # calculate max growth rate of selected cells
num_uniq = length(unique(SortMatrix$Cell_ID))
t_expand = solvet(g = SortMax_gr, N0 = sum(SortMatrix$counts), Nfinal = 0.85*k_96well, k = k_96well)   # solve logistic growth equation for expansion time
### EXPAND CELLS
# (4) Simulate growth of each subclone to t_expand and store new cell counts and abundance in data frame
SortGrow <- SortMatrix %>% select(-abund) %>%
mutate(counts = floor(solveN(g = gr, N0 = counts, tf = t_expand, k = k_96well)), # solve logistic growth for N at expansion time for each Cell_ID
abund = counts/sum(counts))                                              # determine abundance of each Cell_ID in population
head(SortGrow)
print(paste("total number of cells after expansion = ",sum(SortGrow$counts)))
# (5) Stochastic selection of cells that die during expansion to new vessel
cells_lost_expand = sample(    x = SortGrow$Cell_ID,                                     # choose from set of Cell_IDs in data frame
size = round(prob_passage_death*sum(SortGrow$counts)),         # select number of cells equal to current number of cells multiplied by probability of death post cell sort (prob_cellsort_death)
prob = SortGrow$abund,                                         # weight probability of Cell_ID selection by abundance
replace = TRUE)
#SortOut <- SortGrow[SortGrow$Cell_ID %not in% cells_lost_expand, ]                       # store selected cells in new data frame
# count occurences of each cell_ID in cells_lost_expand
num_lost <- data.frame(table(cells_lost_expand))                 # count occurrences of each cell id
colnames(num_lost) <- c('Cell_ID', 'num_cells_lost')             # rename columns
num_lost$Cell_ID <- as.integer(as.character(num_lost$Cell_ID)) # change data type of cell_id column to intege
update_counts <- left_join(SortGrow, num_lost)
update_counts$num_cells_lost[is.na(update_counts$num_cells_lost)] <- 0
SortOut <- update_counts %>% select(-abund) %>% mutate(counts = counts - num_cells_lost) %>% mutate(abund = counts/sum(counts) ) %>% select(-num_cells_lost)
print(paste("total number of cells after replating = ",sum(SortOut$counts)))
SortOut <- update_counts %>% select(-abund) %>% mutate(counts = counts - num_cells_lost) %>%
mutate(abund = counts/sum(counts) ) %>%
select(-num_cells_lost)
# library(ggthemes)
library(dplyr)
library(ggplot2)
# library(tidyverse)
library(tibble)
library(deSolve)
library(lsa)
library(gridExtra)
set.seed(125)
# function to solve an exponential growth equation for the final number of cells given growth rate, starting number of cells, and the final time point
solveN <- function(g, N0, tfinal){
Nfinal <- N0*exp(g*24*tfinal)
return(Nfinal)
}
# function to solve an exponential growth equation for the final time given growth rate, starting number of cells, and the final number of cells
solvet <- function(g, N0, Nfinal){
tfinal <- (1/(g))*log(Nfinal/N0)/24
return(tfinal)
}
# function to normalize columns of data between 0 and 1
normalized<-function(y) {
x <- y[!is.na(y)]
x <- (x - min(x)) / (max(x) - min(x))
y[!is.na(y)]<-x
return(y)
}
## Set known parameters
mean_g <- 0.02   # mean growth rate
sd_g <- 0.004  # expected standard deviation of growth rate
num_reps <- 4*60   # number of replicates
rep_cells <- 1000   # number of cells per replicate
freeze0 <- 1.5e6  # number of cells wanted to freezeback at timepoint 0
other0 <- 0      # number of cells needed for other purposes at timepoint 0
iterations <- 5  # number of times to repeat the simulation
## Choose which diversity libraries to test
test_div <- c(100, 200, 300, 400, 500, 600, 700, 800, 900, 1000)
#test_div <- c(100, 500, 1000)
# This function returns:
## (1) Shannon index of expanded population (SI),
## (2) Mean abundance-corrected Jaccard index between each replicate and the expanded mixed population (JI),
## (3) Mean Euclidean distance (not used in analysis, but is similar to Jaccard) (EU),
## (4) Mean Cosine similarity of replicates (cos_score),
## (5) Mean Shannon diversity of all replicates (shaj)
# ----
metrics <- function(diversity, mean_g, sd_g, num_reps, rep_cells){
# how many cells to expand to based on average growth rate,
# of replicates and size of freezeback at timepoint 0
Nf <-
ceiling(4 * num_reps * rep_cells) + freeze0 + other0
# determine time to reach final number of cells (Nf) assuming mean growth rate
tf <-
solvet(g = mean_g, N0 = diversity, Nfinal = Nf)
# GFP population
counts <-
data.frame('cell' = c(seq(1, diversity))) %>%
# randomly assign growth rates from normal distribution
add_column(gr = rnorm(nrow(.), mean = mean_g, sd = sd_g)) %>%
# solve for N at final timepoint (tf) determined above
mutate(Nfinal = floor(solveN(
g = gr, N0 = 1, tfinal = tf
))) %>%
# compute Shannon Index of initial expanded population
mutate(pi = Nfinal / (sum(Nfinal)), SI = -pi * log(pi))
counts$cell <- as.character(counts$cell)
# calculate Shannon diversity of expanded population for return
shannon <- sum(counts$SI)
# there was a previous error in my random draw, it is fixed here
# TODO: Remove seed for random draws
set.seed(1234)
# random draw from PDF of abundance per cell and rearrange from vector to matrix
draw <-  sample(counts$cell,size = num_reps * rep_cells,replace = TRUE, prob = counts$pi)
# sample from clonal counts estimated in expanded population at final time point (tfinal)
# Each row is a cell, each column is a replicate well w/ number of cells/replicate
draw_mat <-  matrix(draw, ncol = num_reps, nrow = rep_cells)
# TODO: Replace everything with my name in it
stor <- vector(mode = "list", length = num_reps)
cos_jost2 = vector(mode = "list", length = num_reps)
for (i in seq(1, num_reps)) {
# TODO: Decide if the extra 30 seconds is worth it
# Simulate cell growth
# OPTION 1: Get a table (i.e. frequency) of cell number for each replicate
cellsDraw = table(factor(draw_mat[, i], levels = unique(counts$cell)))
noiseLvl = 0.025
# OPTION 2: Take probabilities with random noise added
# cellsDraw = floor(1000*counts$pi*runif(100,min=1-noiseLvl,max=1+noiseLvl))
names(cellsDraw) = counts$cell
cos_jost2[[i]] = cellsDraw
new_mat <- counts
# cellsDraw = table(factor(draw_mat[, i], levels = unique(counts$cell)))
new_mat$num_samp = cellsDraw
# new_mat$num_samp[is.na(new_mat$num_samp)] <-
#   0 # assign NA values from left_join to zero abundance
# TODO: Ask Didi what everything is
# Calculate numerical parameters
pj = new_mat$num_samp / (sum(new_mat$num_samp))
SIj = -pj * log(pj)
SIj[is.na(SIj)] = 0
UV = new_mat$pi * pj
U = new_mat$pi
V = pj
J = UV / ((U ^ 2) + (V ^ 2) - UV)
Euc = (new_mat$pi - pj) ^ 2
# abundance corrected Jaccard index
JI = sum(J)
# total Euclidean distance of replicate population from parental population
EU = sqrt(sum(Euc))
# Shannon index
shannonj = sum(SIj)
# each row here is the similarity score of one replicate to the total mixed population (higher is better)
JIEU <- cbind(JI, EU, shannonj)
stor[[i]] <-
JIEU
}
# Calculate cosine score
cos_jost2 = do.call('cbind', cos_jost2)
cos_jost2 <- cosine(cos_jost2)
cos_scorejost <-
mean(cos_jost2[upper.tri(cos_jost2) == TRUE])
stor = do.call("rbind", stor)
JI <- mean(stor[,1])
Eu <- mean(stor[,2])
shaj <- mean(stor[,3])
return(c(shannon, JI, Eu, cos_scorejost, shaj))
}
# GFP population
counts <-
data.frame('cell' = c(seq(1, diversity))) %>%
# randomly assign growth rates from normal distribution
add_column(gr = rnorm(nrow(.), mean = mean_g, sd = sd_g)) %>%
# solve for N at final timepoint (tf) determined above
mutate(Nfinal = floor(solveN(
g = gr, N0 = 1, tfinal = tf
))) %>%
# compute Shannon Index of initial expanded population
mutate(pi = Nfinal / (sum(Nfinal)), SI = -pi * log(pi))
# GFP population
counts <-
data.frame('cell' = c(seq(1, diversity))) %>%
# randomly assign growth rates from normal distribution
add_column(gr = rnorm(nrow(.), mean = mean_g, sd = sd_g)) %>%
# solve for N at final timepoint (tf) determined above
mutate(Nfinal = floor(solveN(
g = gr, N0 = 1, tfinal = tf
))) %>%
# compute Shannon Index of initial expanded population
mutate(pi = Nfinal / (sum(Nfinal)), SI = -pi * log(pi))
## Set known parameters
mean_g <- 0.02   # mean growth rate
sd_g <- 0.004  # expected standard deviation of growth rate
num_reps <- 4*60   # number of replicates
rep_cells <- 1000   # number of cells per replicate
freeze0 <- 1.5e6  # number of cells wanted to freezeback at timepoint 0
other0 <- 0      # number of cells needed for other purposes at timepoint 0
iterations <- 5  # number of times to repeat the simulation
## Choose which diversity libraries to test
test_div <- c(100, 200, 300, 400, 500, 600, 700, 800, 900, 1000)
#test_div <- c(100, 500, 1000)
# This function returns:
## (1) Shannon index of expanded population (SI),
## (2) Mean abundance-corrected Jaccard index between each replicate and the expanded mixed population (JI),
## (3) Mean Euclidean distance (not used in analysis, but is similar to Jaccard) (EU),
## (4) Mean Cosine similarity of replicates (cos_score),
## (5) Mean Shannon diversity of all replicates (shaj)
# ----
metrics <- function(diversity, mean_g, sd_g, num_reps, rep_cells){
# how many cells to expand to based on average growth rate,
# of replicates and size of freezeback at timepoint 0
Nf <-
ceiling(4 * num_reps * rep_cells) + freeze0 + other0
# determine time to reach final number of cells (Nf) assuming mean growth rate
tf <-
solvet(g = mean_g, N0 = diversity, Nfinal = Nf)
# GFP population
counts <-
data.frame('cell' = c(seq(1, diversity))) %>%
# randomly assign growth rates from normal distribution
add_column(gr = rnorm(nrow(.), mean = mean_g, sd = sd_g)) %>%
# solve for N at final timepoint (tf) determined above
mutate(Nfinal = floor(solveN(
g = gr, N0 = 1, tfinal = tf
))) %>%
# compute Shannon Index of initial expanded population
mutate(pi = Nfinal / (sum(Nfinal)), SI = -pi * log(pi))
counts$cell <- as.character(counts$cell)
# calculate Shannon diversity of expanded population for return
shannon <- sum(counts$SI)
# there was a previous error in my random draw, it is fixed here
# TODO: Remove seed for random draws
set.seed(1234)
# random draw from PDF of abundance per cell and rearrange from vector to matrix
draw <-  sample(counts$cell,size = num_reps * rep_cells,replace = TRUE, prob = counts$pi)
# sample from clonal counts estimated in expanded population at final time point (tfinal)
# Each row is a cell, each column is a replicate well w/ number of cells/replicate
draw_mat <-  matrix(draw, ncol = num_reps, nrow = rep_cells)
# TODO: Replace everything with my name in it
stor <- vector(mode = "list", length = num_reps)
cos_jost2 = vector(mode = "list", length = num_reps)
for (i in seq(1, num_reps)) {
# TODO: Decide if the extra 30 seconds is worth it
# Simulate cell growth
# OPTION 1: Get a table (i.e. frequency) of cell number for each replicate
cellsDraw = table(factor(draw_mat[, i], levels = unique(counts$cell)))
noiseLvl = 0.025
# OPTION 2: Take probabilities with random noise added
# cellsDraw = floor(1000*counts$pi*runif(100,min=1-noiseLvl,max=1+noiseLvl))
names(cellsDraw) = counts$cell
cos_jost2[[i]] = cellsDraw
new_mat <- counts
# cellsDraw = table(factor(draw_mat[, i], levels = unique(counts$cell)))
new_mat$num_samp = cellsDraw
# new_mat$num_samp[is.na(new_mat$num_samp)] <-
#   0 # assign NA values from left_join to zero abundance
# TODO: Ask Didi what everything is
# Calculate numerical parameters
pj = new_mat$num_samp / (sum(new_mat$num_samp))
SIj = -pj * log(pj)
SIj[is.na(SIj)] = 0
UV = new_mat$pi * pj
U = new_mat$pi
V = pj
J = UV / ((U ^ 2) + (V ^ 2) - UV)
Euc = (new_mat$pi - pj) ^ 2
# abundance corrected Jaccard index
JI = sum(J)
# total Euclidean distance of replicate population from parental population
EU = sqrt(sum(Euc))
# Shannon index
shannonj = sum(SIj)
# each row here is the similarity score of one replicate to the total mixed population (higher is better)
JIEU <- cbind(JI, EU, shannonj)
stor[[i]] <-
JIEU
}
# Calculate cosine score
cos_jost2 = do.call('cbind', cos_jost2)
cos_jost2 <- cosine(cos_jost2)
cos_scorejost <-
mean(cos_jost2[upper.tri(cos_jost2) == TRUE])
stor = do.call("rbind", stor)
JI <- mean(stor[,1])
Eu <- mean(stor[,2])
shaj <- mean(stor[,3])
return(c(shannon, JI, Eu, cos_scorejost, shaj))
}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library_size = 1000        # number of barcoded cells sorted/intended library size [number of cells]
mean_growth_rate = 0.022   # mean growth rate of cell line to be used [1/hr]
cell_doubl_time = NA       # estimated cell doubling time [hr]
final_cell_count = 10e6    # number of cells needed for experiments and freezebacks
num_passages = 3           # number of expansions
sd_growth_rate = 0.003        # standard deviation around mean growth rate [1/hr]
k_96well = 55e3               # carrying capacity of a 96-well plate [number of cells]
prob_cellsort_death = 0.15    # fraction of cell death due to cell sort [ ]
prob_passage_death = 0.10     # fraction of cells lost during routine passaging or vessel change [ ]
prob_freezethaw_death = 0.15  # fraction of cells lost due to freeze/thaw cycle []]
### SORT CELLS
# (1) Setup initial cell count matrix with number of cells equal to the number of cells sorted
OGcounts <- data.frame("Cell_ID"= c(seq(1,library_size))) %>%                             # create matrix with cells equal to sorted library size
add_column(    gr = rnorm(nrow(.), mean=mean_growth_rate, sd=sd_growth_rate), # sample growth rates from normal distribution
counts = c(rep(1,library_size)) ) %>%                              # set initial count of each subclone to 1
mutate(abund = counts/sum(counts))                                            # add new column for subclonal abundance
# (2) Stochastic selection of cells that die post-cell sort
SortMatrix <- random_cell_loss(df = OGcounts, prob_death = prob_cellsort_death, replace = FALSE)
`%not in%` <- negate(`%in%`)
### LOGISTIC GROWTH, NUMBER ###
# function to solve a logistic growth equation for the final number of cells given growth rate, starting number of cells, carrying capacity and the final time point
solveN <- function(g, N0, tfinal, k)
{
Nfinal <- (k*N0)/(N0 + ((k-N0)*exp(-g*tfinal)) )
return(Nfinal)
}
### LOGISTIC GROWTH, TIME ###
# function to solve a logistic growth equation for the final time given growth rate, starting number of cells, carrying capacity, and the final number of cells
solvet <- function(g, N0, Nfinal, k)
{
tfinal <- -(log((N0*(1-(k/Nfinal)))/(N0-k)))/g
return(tfinal)
}
### LOGISITC GROWTH PER ROW IN DATA FRAME ###########
# Determine time to reach number of cells equivalent to 80% vessel capacity from known starting number of cells (t_expand)
# Then simulate growth of each subclone to t_expand and store new cell counts and abundance in data frame, return new data frame
grow_cells <- function(df, K, t_expand)
{
max_gr = max(df$gr)                                                                 # calculate max growth rate of selected cells
if(missing(t_expand))
{
t_expand = solvet(g = max_gr, N0 = sum(df$counts), Nfinal = 0.80*K, k = K)      # solve logistic growth equation for expansion time
}
df_out <- df %>% select(-abund) %>%
mutate(counts = floor(solveN(g = gr, N0 = counts, tf = t_expand, k = K)),   # solve logistic growth for N at expansion time for each Cell_ID
abund = counts/sum(counts))                                          # determine abundance of each Cell_ID in population
df_out <- df_out[df_out$counts > 0, ]                                               # remove rows with 0 or less counts
rownames(df_out) <- NULL
return(df_out)
}
### STOCHASTIC SELECTION AT PASSAGE/EXPANSION/SORT/FREEZE/THAW ######
random_cell_loss <- function(df, prob_death, replace = TRUE)
{
if(replace == TRUE)
{
cells_lost  = sample(    x = df$Cell_ID,                                  # choose from set of Cell_IDs in data frame
size = round(prob_death*sum(df$counts)),             # select number of cells equal to current number of cells multiplied by probability of death post cell sort (prob_cellsort_death)
prob = df$abund,                                     # weight probability of Cell_ID selection by abundance
replace = TRUE)
}
if(replace == FALSE)
{
cells_lost  = sample(    x = df$Cell_ID,                                  # choose from set of Cell_IDs in data frame
size = round(prob_death*sum(df$counts)),             # select number of cells equal to current number of cells multiplied by probability of death post cell sort (prob_cellsort_death)
prob = df$abund,                                     # weight probability of Cell_ID selection by abundance
replace = FALSE)
}
num_lost           <- data.frame(table(cells_lost))                              # count occurrences of each cell id in cells_lost_expand
colnames(num_lost) <- c('Cell_ID', 'num_cells_lost')                             # rename columns
num_lost$Cell_ID   <- as.integer(as.character(num_lost$Cell_ID))                 # change data type of cell_id column to integer
update_counts <- left_join(df, num_lost)                                         # join with original data frame with current cell counts
update_counts$num_cells_lost[is.na(update_counts$num_cells_lost)] <- 0
df_out <- update_counts %>% select(-abund) %>%                                   # remove old abundance data
mutate(counts = counts - num_cells_lost) %>%         # calculate new counts by subtracting num_cells_lost occurrences from previous count
mutate(abund = counts/sum(counts) ) %>%              # calculate new abundance data
select(-num_cells_lost)                              # remove unnecessary columns
df_out <- df_out[df_out$counts > 0, ]                                            # remove rows with 0 or less counts
rownames(df_out) <- NULL
return(df_out)
}
### SORT CELLS
# (1) Setup initial cell count matrix with number of cells equal to the number of cells sorted
OGcounts <- data.frame("Cell_ID"= c(seq(1,library_size))) %>%                             # create matrix with cells equal to sorted library size
add_column(    gr = rnorm(nrow(.), mean=mean_growth_rate, sd=sd_growth_rate), # sample growth rates from normal distribution
counts = c(rep(1,library_size)) ) %>%                              # set initial count of each subclone to 1
mutate(abund = counts/sum(counts))                                            # add new column for subclonal abundance
# (2) Stochastic selection of cells that die post-cell sort
SortMatrix <- random_cell_loss(df = OGcounts, prob_death = prob_cellsort_death, replace = FALSE)
### EXPAND CELLS
# (3) Simulate growth of each subclone to 80% vessel carrying capacity and store new cell counts and abundance in data frame
SortGrow <- grow_cells(df = SortMatrix, K = k_96well)
SortOut <- random_cell_loss(df = SortGrow, prob_death = prob_passage_death)
print(paste("total number of cells after first expansion:  ",sum(SortGrow$counts), "cells"))
print(paste("total number of cells after first replating:  ",sum(SortOut$counts), "cells"))
max_gr = max(SortOut$gr)                                                                                  # take SortOut as input and determine time (t_expand) to get to final_cell_count based on max gr in data frame
t_expand = solvet(g = max_gr, N0 = sum(SortOut$counts), Nfinal = final_cell_count, k = 1.2*final_cell_count)# solve logistic growth equation for expansion time
t_interval = t_expand/(num_passages-1)                                                                        # Divide time into intervals, i.e. t_interval =  t_expand / num_passages-1
FinalCounts <- SortOut                                                                                        # Initialize input for looping function
for (i in seq(1,(num_passages-1)))                                                                            # Simulate logistic growth and stochastic cells loss num_passages-1 times over t_interval
{
NewCounts <- grow_cells(df = FinalCounts, K = 1.2*final_cell_count, t_expand = t_interval)
NewCounts <- random_cell_loss(df = NewCounts, prob_death = prob_passage_death)
FinalCounts <- NewCounts
}
print(paste("total number of cells before expansion:  ",sum(SortGrow$counts), "cells"))
print(paste("total number of cells after", round(t_expand/24,2), " days of expansion with ", num_passages-1, "additional vessel changes:  ", round(sum(FinalCounts$counts)/1e6,2), "M cells"))
# How many unique barcodes were lost compared to what was expected
num_cells_experiment <- 2e6
num_cells_freeze <- sum(FinalCounts$counts) - num_cells_experiment
cell_per_freezeback <- 1e6
num_freezbacks <- floor(num_cells_freeze/cell_per_freezeback)
long_cells <- FinalCounts %>% select(-abund, -gr) %>% uncount(counts) # create a list of all cells in the population
mixed_population <- sample(long_cells$Cell_ID, replace = FALSE)   # randomize the list so it's not like 1 1 1 1 2 2 2 2 2 3 3 3 3, but more like 52 100 899 4 37 13
experimental_population <- mixed_population[1:num_cells_experiment]   # grab the first portion of the random list as the experimental population
frozen_populations <- mixed_population[1+num_cells_experiment:length(mixed_population)] # let the rest be frozen, can be further subsetted to individual vials for other analysis
writeLines(paste("                  Sorted library size:",   library_size,
"\nFinal library size before freezebacks:", length(unique(FinalCounts$Cell_ID)),
"\n Experimental population library size:", length(unique(experimental_population))) )
exp_pop           <- data.frame(table(experimental_population))                              # count occurrences of each cell id in cells_lost_expand
colnames(exp_pop) <- c('Cell_ID', 'counts')                             # rename columns
exp_pop$Cell_ID   <- as.integer(as.character(exp_pop$Cell_ID))                 # change data type of cell_id column to integer
exp_pop <- exp_pop %>% mutate(abund = counts/sum(counts) )
long_cells <- FinalCounts %>% select(-abund, -gr) %>% uncount(counts) # create a list of all cells in the population
View(long_cells)
mixed_population <- sample(long_cells$Cell_ID, replace = FALSE)   # randomize the list so it's not like 1 1 1 1 2 2 2 2 2 3 3 3 3, but more like 52 100 899 4 37 13
experimental_population <- mixed_population[1:num_cells_experiment]   # grab the first portion of the random list as the experimental population
frozen_populations <- mixed_population[1+num_cells_experiment:length(mixed_population)] # let the rest be frozen, can be further subsetted to individual vials for other analysis
writeLines(paste("                  Sorted library size:",   library_size,
"\nFinal library size before freezebacks:", length(unique(FinalCounts$Cell_ID)),
"\n Experimental population library size:", length(unique(experimental_population))) )
exp_pop           <- data.frame(table(experimental_population))                              # count occurrences of each cell id in cells_lost_expand
colnames(exp_pop) <- c('Cell_ID', 'counts')                             # rename columns
exp_pop$Cell_ID   <- as.integer(as.character(exp_pop$Cell_ID))                 # change data type of cell_id column to integer
exp_pop <- exp_pop %>% mutate(abund = counts/sum(counts) )
exp_pop
sum(exp_pop$abund)
# Shannon Index (Lan)
shannon <- -sum(exp_pop$abund*ln(exp_pop$abund))
# Shannon Index (Lan)
shannon <- -sum(exp_pop$abund*log(exp_pop$abund))
# Shannon Index (Lan)
exp_pop$abund*log(exp_pop$abund
# Shannon Index (Lan)
try <- exp_pop$abund*log(exp_pop$abund
# Shannon Index (Lan)
exp_pop$abund*log(exp_pop$abund)
exp_pop$abund
# Shannon Index (Lan)
shannon <- sum(exp_pop$abund*log(exp_pop$abund))
# Shannon Index (Lan)
shannon <- -sum(exp_pop$abund*log(exp_pop$abund))
# Shannon Index (Lan)
shannon_exp_pop <- -sum(exp_pop$abund*log(exp_pop$abund))
remove shannon
delete shannon
clear shannon
remove(shannon)
# How many unique barcodes were lost compared to what was expected
num_cells_experiment <- 2e6
num_cells_freeze <- sum(FinalCounts$counts) - num_cells_experiment
cell_per_freezeback <- 1e6
num_freezbacks <- floor(num_cells_freeze/cell_per_freezeback)
long_cells <- FinalCounts %>% select(-abund, -gr) %>% uncount(counts) # create a list of all cells in the population
mixed_population <- sample(long_cells$Cell_ID, replace = FALSE)   # randomize the list so it's not like 1 1 1 1 2 2 2 2 2 3 3 3 3, but more like 52 100 899 4 37 13
experimental_population <- mixed_population[1:num_cells_experiment]   # grab the first portion of the random list as the experimental population
frozen_populations <- mixed_population[1+num_cells_experiment:length(mixed_population)] # let the rest be frozen, can be further subsetted to individual vials for other analysis
writeLines(paste("                  Sorted library size:",   library_size,
"\nFinal library size before freezebacks:", length(unique(FinalCounts$Cell_ID)),
"\n Experimental population library size:", length(unique(experimental_population))) )
exp_pop           <- data.frame(table(experimental_population))                              # count occurrences of each cell id in cells_lost_expand
colnames(exp_pop) <- c('Cell_ID', 'counts')                             # rename columns
exp_pop$Cell_ID   <- as.integer(as.character(exp_pop$Cell_ID))                 # change data type of cell_id column to integer
exp_pop <- exp_pop %>% mutate(abund = counts/sum(counts) )
fro_pop <- data.frame(table(frozen_populations))                        # copied above for frozen_populations
colnames(fro_pop) <- c('Cell_ID', 'counts')                             # rename columns
fro_pop$Cell_ID   <- as.integer(as.character(fro_pop$Cell_ID))
fro_pop <- fro_pop %>% mutate(abund = counts/sum(counts) )
shannon_fro_pop <- -sum(fro_pop$abund*log(fro_pop$abund))
shannon_FinalCounts <- -sum(FinalCounts$abund*log(FinalCounts$abund))
View(OGcounts)
View(FinalCounts)
shannon_OGcounts <- -sum(OGcounts$abund*log(OGcounts$abund))
shannon_OGcounts <- -sum(OGcounts$abund*log(OGcounts$abund))
shannon_SortMatrix <- -sum(SortMatrix$abund*log(SortMatrix$abund))
shannon_SortGrow <- -sum(SortGrow$abund*log(SortGrow$abund))
shannon_SortOut <- -sum(SortOut$abund*log(SortOut$abund))
remove(shannon_SortGrow)
