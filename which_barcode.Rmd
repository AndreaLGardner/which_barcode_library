---
title: "Barcode Diversity"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(vegan)
library(lsa)
library(ggthemes)
```

############ USER INPUTS ####################################
```{r}
library_size = c(100, 500, 1000)        # number of barcoded cells sorted/intended library size [number of cells]
mean_growth_rate = 0.022   # mean growth rate of cell line to be used [1/hr]
cell_doubl_time = NA       # estimated cell doubling time [hr]
final_cell_count = 6e6     # number of cells needed for experiments and freezebacks
num_passages = 3           # number of expansions
num_cells_experiment = 1e6
 cell_per_freezeback = 1e6

```


############ SET INPUTS ####################################
```{r}
iterations = 3 # iterations per library size
sd_growth_rate = 0.004        # standard deviation around mean growth rate [1/hr]
k_96well = 55e3               # carrying capacity of a 96-well plate [number of cells]
prob_cellsort_death = 0.15    # fraction of cell death due to cell sort [ ]
prob_passage_death = 0.10     # fraction of cells lost during routine passaging or vessel change [ ]
prob_freezethaw_death = 0.15  # fraction of cells lost due to freeze/thaw cycle []]
prob_firstpassage_death=0.05  # fraction of complete cell lost due to 1st passage
```


######## SET SIMPLE FUNCTIONS ##############################
```{r}
#################################################### 
`%not in%` <- negate(`%in%`)

#################################################### 
### LOGISTIC GROWTH, NUMBER ###
# function to solve a logistic growth equation for the final number of cells given growth rate, starting number of cells, carrying capacity and the final time point
  solveN <- function(g, N0, tfinal, k)
    {
      Nfinal <- (k*N0)/(N0 + ((k-N0)*exp(-g*tfinal)) )
      return(Nfinal)
    }

#################################################### 
### LOGISTIC GROWTH, TIME ###
# function to solve a logistic growth equation for the final time given growth rate, starting number of cells, carrying capacity, and the final number of cells
  solvet <- function(g, N0, Nfinal, k)
    {
      tfinal <- -(log((N0*(1-(k/Nfinal)))/(N0-k)))/g  
      return(tfinal)
    }

  
#################################################### 
### LOGISITC GROWTH PER ROW IN DATA FRAME ###########
# Determine time to reach number of cells equivalent to 80% vessel capacity from known starting number of cells (t_expand)
# Then simulate growth of each subclone to t_expand and store new cell counts and abundance in data frame, return new data frame
  grow_cells <- function(df, K, t_expand)
    {   
      max_gr = max(df$gr)                                                                 # calculate max growth rate of selected cells
      if(missing(t_expand))
        {
          t_expand = solvet(g = max_gr, N0 = sum(df$counts), Nfinal = 0.80*K, k = K)      # solve logistic growth equation for expansion time
        }  
      df_out <- df %>% select(-abund) %>%  
              mutate(counts = floor(solveN(g = gr, N0 = counts, tf = t_expand, k = K)),   # solve logistic growth for N at expansion time for each Cell_ID        
                     abund = counts/sum(counts))                                          # determine abundance of each Cell_ID in population
      df_out <- df_out[df_out$counts > 0, ]                                               # remove rows with 0 or less counts
      
      rownames(df_out) <- NULL
      return(df_out)
    }

#################################################### 
### STOCHASTIC SELECTION AT PASSAGE/EXPANSION/SORT/FREEZE/THAW ######
  random_cell_loss <- function(df, prob_death, replace = TRUE)
    {
      if(replace == TRUE)
        {
          cells_lost  = sample(        x = df$Cell_ID,                                   # choose from set of Cell_IDs in data frame
                                    size = round(prob_death*sum(df$counts)),             # select number of cells equal to current number of cells multiplied by probability of death post cell sort (prob_cellsort_death)
                                    prob = df$abund,                                     # weight probability of Cell_ID selection by abundance
                                 replace = TRUE) 
      }
      if(replace == FALSE)
        {
          cells_lost  = sample(        x = df$Cell_ID,                                  # choose from set of Cell_IDs in data frame
                                    size = round(prob_death*sum(df$counts)),            # select number of cells equal to current number of cells multiplied by probability of death post cell sort (prob_cellsort_death)
                                    prob = df$abund,                                    # weight probability of Cell_ID selection by abundance
                                 replace = FALSE) 
        }
                                                     
    
      num_lost           <- data.frame(table(cells_lost))                              # count occurrences of each cell id in cells_lost_expand
      colnames(num_lost) <- c('Cell_ID', 'num_cells_lost')                             # rename columns
      num_lost$Cell_ID   <- as.integer(as.character(num_lost$Cell_ID))                 # change data type of cell_id column to integer
        
      
      update_counts <- left_join(df, num_lost)                                         # join with original data frame with current cell counts
      update_counts$num_cells_lost[is.na(update_counts$num_cells_lost)] <- 0
      
      df_out <- update_counts %>% select(-abund) %>%                                   # remove old abundance data
                                  mutate(counts = counts - num_cells_lost) %>%         # calculate new counts by subtracting num_cells_lost occurrences from previous count
                                  mutate(abund = counts/sum(counts) ) %>%              # calculate new abundance data
                                  select(-num_cells_lost)                              # remove unnecessary columns
      df_out <- df_out[df_out$counts > 0, ]                                            # remove rows with 0 or less counts
      
      rownames(df_out) <- NULL
      return(df_out)
    }

####################################################  
#Function calculates Bray Curtis dissimilarity between replicates given the long form of the  cell population of interest and the number of cells per replicate wanted
  BrayCurtisIndex <- function(long_population, cells_per_rep)
      {   
        all_replicate<- data.frame(matrix(ncol = 0, nrow = 0 )) #create empty matrix 
        split_replicates <- split(long_population, ceiling(seq_along(long_population)/cells_per_rep)) #splits cell population of interest evenly by # of cells per aliquot/rep
        for (chunk in split_replicates)
        {
          replicateN<- data.frame(table(chunk)) #creates data.frame from each split
          colnames(replicateN)<- c("Cell_ID", "counts") #rename columns
          replicateN <- replicateN %>% spread(Cell_ID, counts) #create long data frame (row=aliquot col=Cell_ID)
          all_replicate<- bind_rows(all_replicate, replicateN) #combines all replicates into one data frame
        }
        
        all_replicate[is.na(all_replicate)==TRUE ]<-0 #replace all NA values with 0
        
        mean_BCIndex<- mean(vegdist(all_replicate,'bray')) #calculate mean Bray Curtis
        
        
        return(mean_BCIndex)
  }

#################################################### 
# compute shannon diversity index
shannon <- function(df)
  { 
  SI <- -sum( df$abund*log(df$abund)) 
  return(SI)
  }



```

```{r}
# set random seed for first simulation
set.seed(1234)

# setup empty data frame to store simulation outputs
#df <- data.frame(library_size = NA, final_cell_count = NA, SI_experimental = NA, replicate = NA)

res <- data.frame( library_size = NA, 
               final_cell_count = NA,
           final_total_evenness = NA,
                 final_total_SI = NA,
              cells_per_aliquot = NA,
             number_freezebacks = NA,
          experimental_evenness = NA,
                experimental_SI = NA,
                experimental_BC = NA,
                     cells10_BC = NA,
                    cells100_BC = NA,
                   cells1000_BC = NA,
                  cells10000_BC = NA,
                 cells100000_BC = NA,
                      replicate = NA    )



counts_stor <- data.frame(matrix(ncol = 3+max(library_size), nrow = 0 ))

colnames(counts_stor) <- c('library_size','replicate','phase', seq(1,max(library_size)))

counts_stor[1,] <- NA



for(barcodes in seq(1,length(library_size))){
  for(j in seq(1:iterations))
  {

############################################################
############ SORT PHASE ####################################
############################################################

### SORT CELLS
# (1) Setup initial cell count matrix with number of cells equal to the number of cells sorted
OGcounts <- data.frame("Cell_ID"= c(seq(1,library_size[barcodes]))) %>%                             # create matrix with cells equal to sorted library size
            add_column(    gr = rnorm(nrow(.), mean=mean_growth_rate, sd=sd_growth_rate), # sample growth rates from normal distribution
                       counts = c(rep(1,library_size[barcodes])) ) %>%                              # set initial count of each subclone to 1
            mutate(abund = counts/sum(counts))                                            # add new column for subclonal abundance

# (2) Stochastic selection of cells that die post-cell sort
SortMatrix <- random_cell_loss(df = OGcounts, prob_death = prob_cellsort_death, replace = FALSE)


### EXPAND CELLS
# (3) Simulate growth of each subclone to 80% vessel carrying capacity and store new cell counts and abundance in data frame
  SortGrow <- grow_cells(df = SortMatrix, K = k_96well)
  SortOut <- random_cell_loss(df = SortGrow, prob_death = prob_passage_death)

   
# (4) Complete removal of some colonies due to replating 
      complete_lost = sample(x = SortOut$Cell_ID[SortOut$counts < 200], size = round(prob_firstpassage_death* length(SortOut$Cell_ID)))
           complete_num_lost <- data.frame(table(complete_lost))
 colnames(complete_num_lost) <- c('Cell_ID','complete_cells_lost')
   complete_num_lost$Cell_ID <- as.integer(as.character(complete_num_lost$Cell_ID))
   

  #SortOut <- anti_join(SortOut, complete_num_lost, by = c("Cell_ID" = "Cell_ID"))
  
  SortOut <- SortOut[SortOut$Cell_ID %not in% complete_num_lost$Cell_ID, ]
  rownames(SortOut) <- NULL
   
   
#print(paste("total number of cells after first expansion:  ",sum(SortGrow$counts), "cells"))
#print(paste("total number of cells after first replating:  ",sum(SortOut$counts), "cells"))


############################################################
######### BULK EXPANSION PHASE #############################
############################################################

    max_gr = max(SortOut$gr)                                                                                  # take SortOut as input and determine time (t_expand) to get to final_cell_count based on max gr in data frame
  t_expand = solvet(g = max_gr, N0 = sum(SortOut$counts), Nfinal = final_cell_count, k = 1.2*final_cell_count)# solve logistic growth equation for expansion time
t_interval = t_expand/(num_passages-1)                                                                        # Divide time into intervals, i.e. t_interval =  t_expand / num_passages-1

FinalCounts <- SortOut                                                                                        # Initialize input for looping function
for (i in seq(1,(num_passages-1)))                                                                            # Simulate logistic growth and stochastic cells loss num_passages-1 times over t_interval
  {
    NewCounts <- grow_cells(df = FinalCounts, K = 1.2*final_cell_count, t_expand = t_interval)
    NewCounts <- random_cell_loss(df = NewCounts, prob_death = prob_passage_death)
    FinalCounts <- NewCounts
  }

#print(paste("total number of cells before expansion:  ",sum(SortGrow$counts), "cells"))
#print(paste("total number of cells after", round(t_expand/24,2), " days of expansion with ", num_passages-1, "additional vessel changes:  ", round(sum(FinalCounts$counts)/1e6,2), "M cells"))


############################################################
##### [optional] FREEZE/THAW PHASE #########################
############################################################

# need to split cells into replicate vials each containing 1M cells each, then simulate stochastic loss and then growth on one of those vials

# FreezeOut <- random_cell_loss(df = FinalCounts, prob_death = prob_freezethaw_death)
# print(paste("total number of cells after freeze/thaw:  ",sum(FreezeOut$counts), "cells"))


############################################################
######### EXPERIMENT PHASE #################################
############################################################

# How many unique barcodes were lost compared to what was expected
    num_cells_freeze <- sum(FinalCounts$counts) - num_cells_experiment
      num_freezbacks <- floor(num_cells_freeze/cell_per_freezeback)


long_cells <- FinalCounts %>% select(-abund, -gr) %>% uncount(counts) # create a list of all cells in the population

mixed_population <- sample(long_cells$Cell_ID, replace = FALSE)   # randomize the list so it's not like 1 1 1 1 2 2 2 2 2 3 3 3 3, but more like 52 100 899 4 37 13

experimental_population <- mixed_population[1:num_cells_experiment]   # grab the first portion of the random list as the experimental population
     frozen_populations <- mixed_population[1+num_cells_experiment:length(mixed_population)] # let the rest be frozen, can be further subsetted to individual vials for other analysis

# writeLines(paste("                  Sorted library size:",   library_size[barcodes], 
#                  "\nFinal library size before freezebacks:", length(unique(FinalCounts$Cell_ID)),
#                  "\n Experimental population library size:", length(unique(experimental_population))) )  

exp_pop           <- data.frame(table(experimental_population))                              # count occurrences of each cell id in cells_lost_expand
colnames(exp_pop) <- c('Cell_ID', 'counts')                             # rename columns
exp_pop$Cell_ID   <- as.integer(as.character(exp_pop$Cell_ID))                 # change data type of cell_id column to integer

exp_pop <- exp_pop %>% mutate(abund = counts/sum(counts) )

fro_pop <- data.frame(table(frozen_populations))                        # copied above for frozen_populations
colnames(fro_pop) <- c('Cell_ID', 'counts')                             # rename columns
fro_pop$Cell_ID   <- as.integer(as.character(fro_pop$Cell_ID))
fro_pop <- fro_pop %>% mutate(abund = counts/sum(counts) )


############################################################
###### DIVERSITY AND SIMILARITY METRICS ####################
############################################################

# DIVERSITY METRICS

   shannon_OGcounts <- shannon(OGcounts)     # calculate shannon index of initial cell library before cell sort
 shannon_SortMatrix <- shannon(SortMatrix)   # calculate shannon index after cell sort
    shannon_SortOut <- shannon(SortOut)      # calculate shannon index after first expansion/passage
shannon_FinalCounts <- shannon(FinalCounts)  # calculate shannon index after Bulk expansion
    shannon_exp_pop <- shannon(exp_pop)      # calculate shannon index of experimental population
    shannon_fro_pop <- shannon(fro_pop)      # calculate shannon index of frozen population

# Shannon equitability index, i.e. Evenness
   sei_OGcounts <-    shannon_OGcounts/log(nrow(OGcounts))
 sei_SortMatrix <-  shannon_SortMatrix/log(nrow(SortMatrix))
    sei_SortOut <-     shannon_SortOut/log(nrow(SortOut))
sei_FinalCounts <- shannon_FinalCounts/log(nrow(FinalCounts))
    sei_exp_pop <-     shannon_exp_pop/log(nrow(exp_pop))
    sei_fro_pop <-     shannon_fro_pop/log(nrow(fro_pop))


## BRAY CURTIS INDEX FOR ALIQUOTS
  aliquot_BC <- BrayCurtisIndex(long_population = mixed_population, cells_per_rep = 1e6)

############## compute similarity of replicate

 num_replicates = 5

    cells10_BC <- BrayCurtisIndex(long_population = sample(x = experimental_population, size = num_replicates*10    ), cells_per_rep = 10)
   cells100_BC <- BrayCurtisIndex(long_population = sample(x = experimental_population, size = num_replicates*100   ), cells_per_rep = 100)   
  cells1000_BC <- BrayCurtisIndex(long_population = sample(x = experimental_population, size = num_replicates*1000  ), cells_per_rep = 1000)
 cells10000_BC <- BrayCurtisIndex(long_population = sample(x = experimental_population, size = num_replicates*10000 ), cells_per_rep = 10000) 
cells100000_BC <- BrayCurtisIndex(long_population = sample(x = experimental_population, size = num_replicates*100000), cells_per_rep = 100000)
   
#########################################################################################
######## STORE OUTS PUTS IN DATA FRAME AND RETURN DATA FRAME TO BEGINNING OF LOOP #######
#########################################################################################
# make sure order of outs matches columns of original empty df


outs <- c(       library_size = library_size[barcodes], 
             final_cell_count = sum(FinalCounts$counts),
         final_total_evenness = sei_FinalCounts,
               final_total_SI = shannon_FinalCounts,
            cells_per_aliquot = cell_per_freezeback,
           number_freezebacks = num_freezbacks,
        experimental_evenness = sei_exp_pop,
              experimental_SI = shannon_exp_pop,
              experimental_BC = aliquot_BC,
                   cells10_BC = cells10_BC,
                  cells100_BC = cells100_BC,
                 cells1000_BC = cells1000_BC,
                cells10000_BC = cells10000_BC,
               cells100000_BC = cells100000_BC,
                    replicate = j)



aliquot_counts <- data.frame(library_size = library_size[barcodes], replicate = j, phase = 'aliquot') %>% mutate(exp_pop %>% select(-abund) %>% spread(Cell_ID, counts))

counts_stor <- bind_rows(counts_stor, aliquot_counts)

res <- rbind(res, outs)
  
  }
  
}

res <- res[is.na(res$library_size) == FALSE, ]
rownames(res) <- NULL

counts_stor <- counts_stor[is.na(counts_stor$library_size) == FALSE, ]
rownames(counts_stor) <- NULL
counts_stor[is.na(counts_stor) == TRUE] <- 0

```



############################################################
###### PLOTTING AND ANALYSIS ###############################
############################################################
```{r}

############# EVENNNESS #########

## Evenness of final library
ggplot(res, aes(x = library_size, y = final_total_evenness, group = library_size)) +
  geom_violin(fill='grey') + geom_point() + theme_few() +
  ylim(0.7, 1) +
  ggtitle('Expanded Library Evenness')

## Evenness per aliquot/freezeback/experimental population
ggplot(res, aes(x = library_size, y = experimental_evenness, group = library_size)) +
  geom_violin(fill='grey') + geom_point() + theme_few() +
  ylim(0.7, 1) +
  ggtitle('Experimental Population/Freezeback Evenness')


############# SHANNON DIVERSITY #########

## Diversity of final library 
ggplot(res, aes(x = library_size, y = final_total_SI, group = library_size)) +
  geom_violin(fill='grey') + geom_point() + theme_few() +
  ggtitle('Expanded Library Diversity - Shannon Index')

## Diversity of aliquot/freezeback/experimental population
ggplot(res, aes(x = library_size, y = experimental_SI, group = library_size)) +
  geom_violin(fill='grey') + geom_point() + theme_few() +
  ggtitle('Experimental Population/Freezeback - Shannon Index')

############# BRAY-CURTIS SIMILARITY #########

## Similarity of aliquot/freezeback/experimental populations
ggplot(res, aes(x = library_size, y = experimental_BC, group = library_size)) +
  geom_violin(fill='grey') + geom_point() + theme_few() +
  ggtitle('Experimental Population/Freezeback - Shannon Index')


```

############################################################
###### CLONAL PIE CHARTS ###################################
############################################################
```{r}
# final cell distributions per run
# note that cell #1 in replicate 1 IS NOT the same as cell #1 in replicate 2. These are generated from independent simulations and this have difference mean growth rates

counts_stor

```

